{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae91f5c",
   "metadata": {},
   "source": [
    "---\n",
    "title: Step 04 - Modeling\n",
    "subject: Churn Analysis\n",
    "subtitle: Step 03 - Modeling - Churn Analysis\n",
    "short_title: Modeling\n",
    "date: 2025-12-17\n",
    "\n",
    "affiliations:\n",
    "  - id: \"ucb\"\n",
    "    name: \"University of California, Berkeley\"\n",
    "\n",
    "authors:\n",
    "  - name: Jocelyn Perez\n",
    "    affiliations: [\"ucb\"]\n",
    "    email: jocelyneperez@berkeley.edu\n",
    "    orcid: 0009-0009-0231-9254\n",
    "\n",
    "  - name: Claire Kaoru Shimazaki\n",
    "    affiliations: [\"ucb\"]\n",
    "    email: ckshimazaki@berkeley.edu\n",
    "    orcid: 0009-0001-0828-3370\n",
    "\n",
    "  - name: Colby Zhang\n",
    "    affiliations: [\"ucb\"]\n",
    "    email: colbyzhang@berkeley.edu\n",
    "    orcid: 0009-0005-4786-6922\n",
    "\n",
    "  - name: Olorundamilola Kazeem\n",
    "    affiliations: [\"ucb\"]\n",
    "    email: dami@berkeley.edu\n",
    "    orcid: 0000-0003-2118-2221\n",
    "\n",
    "# https://mystmd.org/guide/frontmatter#frontmatter-downloads\n",
    "# https://mystmd.org/guide/website-downloads\n",
    "# downloads:\n",
    "#   -  ...\n",
    "\n",
    "# https://mystmd.org/guide/website-downloads#include-exported-pdf\n",
    "# exports:\n",
    "#   - format: pdf\n",
    "#     template: lapreprint-typst\n",
    "#     output: exports/my-document.pdf\n",
    "#     id: my-document-export\n",
    "# downloads:\n",
    "#   - id: my-document-export\n",
    "#     title: A PDF of this document\n",
    "\n",
    "exports:\n",
    "  - format: pdf\n",
    "    template: lapreprint-typst\n",
    "    output: ../pdf_builds/step04_modeling/step04_modeling_ipynb_to.pdf\n",
    "    line_numbers: true\n",
    "\n",
    "license: CC-BY-4.0\n",
    "\n",
    "keywords: modeling, churn, spotify\n",
    "\n",
    "abstract: What are the models? How are we modeling?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bca144",
   "metadata": {},
   "source": [
    "In step 03, we translated raw logs into **Behavioral Signals** (e.g. ad tolerance and engagement intensity). The goal of this step is to use these features to predict whether a user will **churn** or **remain active**.\n",
    "\n",
    "In this notebook, we focused on three task:\n",
    "- **Baseline Model**: We start with **Logistic Regression** to establish a baseline level of perofrmance. This model provides a simple, interpretable reference point.\n",
    "- **Class Imbalance**: Churners make up a minority of users, so accuracy alone can be misleading. To prevent the model from defaulting to predicting \"non-churn,\" we apply **class weighting** to account for the imbalance between classes.\n",
    "- **Random Forest Model**: We then train a **Random Forest Classifer** to capture non-linear relationship between behavioral features. This allows the model to learn interaction effecs that a linear model may miss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ecac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import src.step00_utils as step00\n",
    "import src.step03_features as step03\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = Path('../data/02_vectorized')\n",
    "DATA_DIR = step00.DIR_DATA_02_VECTORIZED\n",
    "TRAIN_PATH = DATA_DIR / 'train.joblib'\n",
    "TEST_PATH = DATA_DIR / 'test.joblib'\n",
    "# FIG_DIR = step03.DIR_DATA.parent / \"fig_builds\" / \"step04_modeling\"\n",
    "FIG_DIR = step00.DIR_OUTPUTS_FIG_BUILDS_04_MODELING\n",
    "# set seed\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = joblib.load(TRAIN_PATH)\n",
    "test_data = joblib.load(TEST_PATH)\n",
    "\n",
    "X_train = train_data['X']\n",
    "y_train = train_data['y']\n",
    "\n",
    "X_test = test_data['X']\n",
    "y_test = test_data['y']\n",
    "\n",
    "print(f\"Training Shape: {X_train.shape}\")\n",
    "print(f\"Test Shape:     {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d0f4c",
   "metadata": {},
   "source": [
    "### Baseline Model (Logistic Regression)\n",
    "\n",
    "We begin with **Logistic Regression** to establish a baseline level of performance. This model assumes a linear relationship between the behavioral features (e.g., `ads_per_song`, engagement metrics) and the probability of churn.\n",
    "\n",
    "Because churners represent a minority of users, we use `class_weight='balanced'` to penalize misclassification of the churn class more heavily. This prevents the model from achieving artificially high accuracy by predicting “non-churn” for most users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "baseline = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "baseline.fit(X_train, y_train)\n",
    "print(f\"Baseline Accuracy: {baseline.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a53c01",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "\n",
    "User behavior is unlikely to follow a purely linear pattern. Certain features may only become predictive of churn when combined with others (for example, high ad exposure may matter more for users with low engagement).\n",
    "\n",
    "To capture these non-linear relationships, we train a **Random Forest Classifier**. As an ensemble of decision trees, the Random Forest can model interaction effects that Logistic Regression cannot.\n",
    "\n",
    "We use **GridSearchCV** to tune key hyperparameters, allowing us to balance model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4238c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced')\n",
    "\n",
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(\"\\ngrid search for random forest\")\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nbest params: {grid_search.best_params_}\")\n",
    "print(f\"best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e37fd",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "In churn prediction, accuracy alone is not sufficient. Since most users do not churn, a model that always predicts “non-churn” can achieve high accuracy while providing little practical value.\n",
    "\n",
    "To properly evaluate performance, we examine:\n",
    "- The **classification report**, which summarizes precision, recall, and F1-score for each class\n",
    "- The **Confusion Matrix**, to understand the types of errors the model makes\n",
    "\n",
    "These metrics allow us to assess how well the model identifies churners without excessively misclassifying retained users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75340b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig(FIG_DIR / 'step04_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"age\", \n",
    "    \"listening_time\", \n",
    "    \"songs_played_per_day\", \n",
    "    \"skip_rate\", \n",
    "    \"ads_listened_per_week\", \n",
    "    \"ads_per_song\", \n",
    "    \"avg_song_length\"\n",
    "]\n",
    "\n",
    "# generic labels for categorical variables (e.g. one-hot encoded countries)\n",
    "if X_train.shape[1] > len(feature_names):\n",
    "    feature_names += [f\"Other_Feature_{i}\" for i in range(X_train.shape[1] - len(feature_names))]\n",
    "\n",
    "importances = final_model.feature_importances_\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='importance', y='feature', hue='feature', legend=False, data=feat_imp, palette='viridis')\n",
    "plt.title('Top 10 Features Driving the Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.savefig(FIG_DIR / 'step04_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dfc8a",
   "metadata": {},
   "source": [
    "Our analysis reveals that **behavioral engagement is the primary driver of churn**, significantly outweighing static demographic traits. The model relies mostly heavily on **Average Song Length** and **Listening Time**, indicating that users who listen to songs in their entirely (deep engagement) are far more likely to retain their subscrption than those who frequently skip. **Skip Rate** appears as a critical friction signal, confirming that dissatisfaction with content is a leading indicator of departure. Notably, **Age** is the only demographic factor in the top five, suggesting that while usage habits are dominant, the generational culture and budget constraints assocaited with a user's stage in life are still fundamental predictors of retention.\n",
    "\n",
    "However, despite these insights, our predictive power remains modest (F1-score ~0.25). This aligns with prior research (Verbeke et al., 2012; Ascarza, 2018), which suggests that churn is notoriously difficult to predict. The decision to leave is often driven by **unpredictable events** (like a lost job or a competitor's sudden offer) and **hidden personal factors** that our dataset simply does not capture. These external forces create a \"ceiling\" on how accurate any model can be, confirming that behavioral data is necessary, but not always sufficient, to predict every individual outcome."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "IPython - final-group08",
   "language": "python",
   "name": "final-group08"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
